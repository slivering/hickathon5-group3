{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSEDJ-O5TvrM",
        "outputId": "bc08bc02-aeea-4829-9c2d-5e50ca3fbb3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.1.4\n",
            "1.26.4\n",
            "1.0\n",
            "2.5.1+cu124\n",
            "1.4.2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import sklearn\n",
        "import xgboost as xgb\n",
        "\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(pd.__version__)\n",
        "print(np.__version__)\n",
        "print(csv.__version__)\n",
        "print(torch.__version__)\n",
        "print(sklearn.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_by_chunks(path: str, chunk_size: int = 50_000, **kwargs) -> pd.DataFrame:\n",
        "    chunks = pd.read_csv(path, chunksize=chunk_size, float_precision='round_trip', **kwargs)\n",
        "    return pd.concat(chunks, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wDI4dyxLq5cM"
      },
      "outputs": [],
      "source": [
        "class_mapping = {\n",
        "    \"Very Low\": 0,\n",
        "    \"Low\": 1,\n",
        "    \"Average\": 2,\n",
        "    \"High\": 3,\n",
        "    \"Very High\": 4\n",
        "}\n",
        "\n",
        "inverse_class_mapping = {v: k for k, v in class_mapping.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iALUPz1jq9xm",
        "outputId": "7d2e5e4c-9e56-4520-c02c-d72fa6f7155c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "piezo_groundwater_level_category\n",
            "High         13306\n",
            "Average      11623\n",
            "Very High    11548\n",
            "Low           8246\n",
            "Very Low      5277\n",
            "Name: count, dtype: int64\n",
            "\n",
            "piezo_groundwater_level_category\n",
            "High         0.26612\n",
            "Average      0.23246\n",
            "Very High    0.23096\n",
            "Low          0.16492\n",
            "Very Low     0.10554\n",
            "Name: count, dtype: float64\n",
            "\n",
            "piezo_groundwater_level_category\n",
            "High         3.757703\n",
            "Average      4.301815\n",
            "Very High    4.329754\n",
            "Low          6.063546\n",
            "Very Low     9.475081\n",
            "Name: count, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "train_csv = \"X_train_clean.csv\"\n",
        "test_csv  = \"X_test_clean.csv\"\n",
        "\n",
        "df = pd.read_csv(train_csv, nrows=50_000)\n",
        "\n",
        "class_counts = df['piezo_groundwater_level_category'].value_counts()\n",
        "print(class_counts)\n",
        "\n",
        "# Calculate weights\n",
        "total = class_counts.sum()\n",
        "class_weights = class_counts / total\n",
        "\n",
        "# Print the weights\n",
        "print()\n",
        "print(class_weights)\n",
        "print()\n",
        "print(1/class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d0aOheJPcOmB"
      },
      "outputs": [],
      "source": [
        "def drop_columns(df: pd.DataFrame, threshold=0.2, inplace=False) -> pd.DataFrame | None:\n",
        "    \"\"\"\n",
        "    Drop columns with a fraction of more than `threshold` missing values.\n",
        "    \"\"\"\n",
        "    threshold = int((1.0 - threshold) * len(df))\n",
        "    return df.dropna(axis=1, thresh=threshold, inplace=inplace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qLndFRZjq-Tl"
      },
      "outputs": [],
      "source": [
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, csv_file, label_column=None):\n",
        "        self.data_frame = load_by_chunks(csv_file)\n",
        "        self.label_column = label_column\n",
        "\n",
        "        self.data_frame = self.data_frame.drop(columns=['meteo_radiation_IR'], errors='ignore')\n",
        "\n",
        "        if self.label_column:\n",
        "            self.data_frame[self.label_column] = self.data_frame[self.label_column].astype(str)\n",
        "            self.labels = self.data_frame[self.label_column].map(class_mapping)\n",
        "            self.labels = self.labels.astype(int).values\n",
        "            self.features = self.data_frame.drop(columns=[self.label_column])\n",
        "        else:\n",
        "            self.features = self.data_frame\n",
        "            self.labels = None\n",
        "\n",
        "        self.encoders = {}\n",
        "        for col in self.features.columns:\n",
        "            if self.features[col].dtype == 'object' or not pd.api.types.is_numeric_dtype(self.features[col]):\n",
        "                encoder = LabelEncoder()\n",
        "                self.features[col] = encoder.fit_transform(self.features[col].astype(str))\n",
        "                self.encoders[col] = encoder\n",
        "\n",
        "        self.features = self.features.apply(pd.to_numeric, errors='coerce')\n",
        "        self.features = self.features.fillna(self.features.mean())\n",
        "        self.features = self.features.values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        features = torch.tensor(self.features[idx], dtype=torch.float32)\n",
        "        if self.labels is not None:\n",
        "            label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "            return features, label\n",
        "        return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "51OFma6cq-PI"
      },
      "outputs": [],
      "source": [
        "def vector_to_class(x):\n",
        "  y = torch.argmax(x,axis=1)\n",
        "  return y\n",
        "\n",
        "def prediction_accuracy(predict,labels):\n",
        "  accuracy = (predict == labels).sum()/(labels.shape[0])\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Séparation en données d'entraînement et en données de validation\n",
        "\n",
        "De 30% du jeu d'entraînement (`X_train_Hi5.csv`), on extrait trois jeux de données :\n",
        "- `train_temp.csv`, `val.csv` : données d'entraînement et de validation pour les tests initiaux\n",
        "- `train.csv` : données d'entraînement pour la soumission finale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "3uz6_1WLtScy"
      },
      "outputs": [],
      "source": [
        "def split_csv_by_fraction(\n",
        "        input_file,\n",
        "        train_temp_output,\n",
        "        val_output,\n",
        "        train_output,\n",
        "    ):\n",
        "    chunks = pd.read_csv(input_file, chunksize=50_000, float_precision='round_trip')\n",
        "    # Retenir 30% des données de chaque morceau (chunk).\n",
        "    df = pd.concat(\n",
        "        (train_test_split(chunk, train_size=0.7, random_state=42)[1] for chunk in chunks),\n",
        "        ignore_index=True,\n",
        "    )\n",
        "    df['piezo_measurement_date'] = pd.to_datetime(df['piezo_measurement_date'])\n",
        "    month = df['piezo_measurement_date'].dt.month\n",
        "    year = df['piezo_measurement_date'].dt.year\n",
        "\n",
        "    # été 2020 + mai, octobre 2020-2023 : données d'entraînement pour la validation\n",
        "    train_temp_df = df[\n",
        "        ((year == 2020) & (month >= 6) & (month <= 9))\n",
        "        | ((month == 5) | (month == 10))\n",
        "    ].sample(frac=1, random_state=42)\n",
        "\n",
        "    # été 2021 + avril, novembre 2020-2023 : données de validation pour les tests initiaux\n",
        "    val_df = df[\n",
        "        ((year == 2021) & (month >= 6) & (month <= 9))\n",
        "        | ((month == 4) | (month == 11))\n",
        "    ].sample(frac=1, random_state=42)\n",
        "\n",
        "    # étés 2020-2021 + mai, octobre 2020-2023 : données d'entraînement pour la soumission\n",
        "    train_df = df[\n",
        "        (((year == 2020) | (year == 2021)) & (month >= 6) & (month <= 9))\n",
        "        | ((month == 5) | (month == 10))\n",
        "    ].sample(frac=1, random_state=42)\n",
        "\n",
        "    print(\n",
        "        'Train set for validation:', train_temp_df.shape,\n",
        "        'Validation set:', val_df.shape,\n",
        "        'Train set for submission:', train_df.shape,\n",
        "    )\n",
        "    train_temp_df.to_csv(train_temp_output, index=False)\n",
        "    val_df.to_csv(val_output, index=False)\n",
        "    train_df.to_csv(train_output, index=False)\n",
        "    print(f\"Saved {train_temp_output}, {val_output} and {train_output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uUVYX3ttr0R",
        "outputId": "875d1770-12e0-4747-e717-474b89299319"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set for validation: (255294, 136) Validation set: (251175, 136) Train set for submission: (348042, 136)\n",
            "Saved train_temp.csv, val.csv and train.csv\n"
          ]
        }
      ],
      "source": [
        "split_csv_by_fraction(train_csv, \"train_temp.csv\", \"val.csv\", \"train.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Score de validation pour les tests initiaux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6vpZhESoXUK",
        "outputId": "60280759-24f8-400d-c918-25d987396745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 0.472\n"
          ]
        }
      ],
      "source": [
        "train_csv_temp = \"./train_temp.csv\"\n",
        "val_csv = \"./val.csv\"\n",
        "train_dataset = TabularDataset(csv_file=train_csv_temp, label_column='piezo_groundwater_level_category')\n",
        "val_dataset = TabularDataset(csv_file=val_csv, label_column='piezo_groundwater_level_category')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# TODO: cross-validation max_depth, alpha, learning_rate (0, 1), max_delta_step (0, 10)\n",
        "#objective=\"multi:softprob\"\n",
        "xgb_model = xgb.XGBClassifier(device=\"gpu\", tree_method=\"gpu_hist\", reg_alpha=0.5)\n",
        "\n",
        "xgb_model.fit(train_dataset.features, train_dataset.labels)\n",
        "\n",
        "# Validate the model\n",
        "val_preds = xgb_model.predict(val_dataset.features)\n",
        "val_accuracy = prediction_accuracy(val_dataset.labels, val_preds)\n",
        "print(f'Validation accuracy: {val_accuracy:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entraînement pour la soumission finale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy (sanity check): 0.534\n"
          ]
        }
      ],
      "source": [
        "# Finalement, on n'utilise pas le jeu d'entraînement entier car cela réduit la précision du modèle\n",
        "train_csv = \"./train_temp.csv\"\n",
        "train_dataset = TabularDataset(csv_file=train_csv, label_column='piezo_groundwater_level_category')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# TODO: cross-validation max_depth, alpha, learning_rate (0, 1), max_delta_step (0, 10)\n",
        "#objective=\"multi:softprob\"\n",
        "xgb_model = xgb.XGBClassifier(device=\"gpu\", tree_method=\"gpu_hist\", reg_alpha=0.5)\n",
        "\n",
        "xgb_model.fit(train_dataset.features, train_dataset.labels)\n",
        "\n",
        "# Valider le model (valeur non pertinente, ne sert que pour vérifier que tout se passe bien)\n",
        "val_preds = xgb_model.predict(val_dataset.features)\n",
        "val_accuracy = prediction_accuracy(val_dataset.labels, val_preds)\n",
        "print(f'Validation accuracy (sanity check): {val_accuracy:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_predictions(model, outfile='test_results.csv'):\n",
        "    test_dataset = TabularDataset(csv_file='./X_test_Hi5.csv')\n",
        "    test_preds = model.predict(test_dataset.features)\n",
        "    \n",
        "    # Save results to CSV\n",
        "    decoded_preds = [inverse_class_mapping[pred] for pred in test_preds]\n",
        "    with open(outfile, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        # Write header\n",
        "        writer.writerow(['row_index', 'piezo_groundwater_level_category'])\n",
        "        for id, pred_class in zip(test_dataset.data_frame['row_index'], decoded_preds):\n",
        "            writer.writerow([id, pred_class])  # Write each ID and predicted class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_predictions(xgb_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_model.save_model('xgb_model.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
