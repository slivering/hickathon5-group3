{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfkSt_Ag3Q0M",
        "outputId": "7337ec70-b32d-4217-9d23-eb764b7d7c10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/lvt/.local/share/virtualenvs/ml-TQx6uCZG/bin/python: No module named pip\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "/home/lvt/.local/share/virtualenvs/ml-TQx6uCZG/bin/python: No module named pip\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "/home/lvt/.local/share/virtualenvs/ml-TQx6uCZG/bin/python: No module named pip\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pytorch_tabnet\n",
        "%pip install lightgbm\n",
        "%pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSEDJ-O5TvrM",
        "outputId": "a2e0acc9-3868-498e-c8a8-1edf25d99e17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.1.4\n",
            "1.26.4\n",
            "1.0\n",
            "2.5.1+cu124\n",
            "1.4.2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import sklearn\n",
        "\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(pd.__version__)\n",
        "print(np.__version__)\n",
        "print(csv.__version__)\n",
        "print(torch.__version__)\n",
        "print(sklearn.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_csv = \"./X_train_clean.csv\"\n",
        "train_temp_csv = \"./train_temp.csv\"\n",
        "val_csv = \"./val.csv\"\n",
        "test_csv  = \"./X_test_clean.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_csv_by_fraction(\n",
        "        input_file,\n",
        "        train_temp_output,\n",
        "        val_output,\n",
        "        train_output,\n",
        "    ):\n",
        "    chunks = pd.read_csv(input_file, chunksize=50_000, float_precision='round_trip')\n",
        "    df = pd.concat(\n",
        "        # Extraire 20 % (aléatoire) des données\n",
        "        (chunk.sample(frac=0.2) for chunk in chunks),\n",
        "        ignore_index=True,\n",
        "    )\n",
        "    df['piezo_measurement_date'] = pd.to_datetime(df['piezo_measurement_date'])\n",
        "    month = df['piezo_measurement_date'].dt.month\n",
        "    year = df['piezo_measurement_date'].dt.year\n",
        "\n",
        "    # été 2020 + mai, octobre 2020-2023 : données d'entraînement pour la validation\n",
        "    train_temp_df = df[\n",
        "        ((year == 2020) & (month >= 6) & (month <= 9))\n",
        "        | ((month == 5) | (month == 10))\n",
        "    ].sample(frac=1, random_state=42)\n",
        "\n",
        "    # été 2021 + avril, novembre 2020-2023 : données de validation pour les tests initiaux\n",
        "    val_df = df[\n",
        "        ((year == 2021) & (month >= 6) & (month <= 9))\n",
        "        | ((month == 4) | (month == 11))\n",
        "    ].sample(frac=1, random_state=42)\n",
        "\n",
        "    # étés 2020-2021 + mai, octobre 2020-2023 : données d'entraînement pour la soumission\n",
        "    train_df = df[\n",
        "        (((year == 2020) | (year == 2021)) & (month >= 6) & (month <= 9))\n",
        "        | ((month == 5) | (month == 10))\n",
        "    ].sample(frac=1, random_state=42)\n",
        "\n",
        "    print(\n",
        "        'Train set for validation:', train_temp_df.shape,\n",
        "        'Validation set:', val_df.shape,\n",
        "        'Train set for submission:', train_df.shape,\n",
        "    )\n",
        "    train_temp_df.to_csv(train_temp_output, index=False)\n",
        "    val_df.to_csv(val_output, index=False)\n",
        "    train_df.to_csv(train_output, index=False)\n",
        "    print(f\"Saved {train_temp_output}, {val_output} and {train_output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set for validation: (753470, 64) Validation set: (765993, 64) Train set for submission: (1037140, 64)\n",
            "Saved ./train_temp.csv, ./val.csv and train.csv\n"
          ]
        }
      ],
      "source": [
        "split_csv_by_fraction(train_csv, train_temp_csv, val_csv, \"train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wDI4dyxLq5cM"
      },
      "outputs": [],
      "source": [
        "class_mapping = {\n",
        "    \"Very Low\": 0,\n",
        "    \"Low\": 1,\n",
        "    \"Average\": 2,\n",
        "    \"High\": 3,\n",
        "    \"Very High\": 4\n",
        "}\n",
        "\n",
        "inverse_class_mapping = {v: k for k, v in class_mapping.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iALUPz1jq9xm",
        "outputId": "e869a710-a4ab-4186-8c8e-d7ffb3cb894f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "piezo_groundwater_level_category\n",
            "Very High    113553\n",
            "High         113122\n",
            "Average      107547\n",
            "Low           96607\n",
            "Very Low      69171\n",
            "Name: count, dtype: int64\n",
            "\n",
            "piezo_groundwater_level_category\n",
            "Very High    0.227106\n",
            "High         0.226244\n",
            "Average      0.215094\n",
            "Low          0.193214\n",
            "Very Low     0.138342\n",
            "Name: count, dtype: float64\n",
            "\n",
            "piezo_groundwater_level_category\n",
            "Very High    4.403230\n",
            "High         4.420007\n",
            "Average      4.649130\n",
            "Low          5.175608\n",
            "Very Low     7.228463\n",
            "Name: count, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "df = pd.read_csv(train_csv, nrows=5*10**5)\n",
        "\n",
        "class_counts = df['piezo_groundwater_level_category'].value_counts()\n",
        "print(class_counts)\n",
        "\n",
        "# Calculate weights\n",
        "total = class_counts.sum()\n",
        "class_weights = class_counts / total\n",
        "\n",
        "# Print the weights\n",
        "print()\n",
        "print(class_weights)\n",
        "print()\n",
        "print(1/class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qLndFRZjq-Tl"
      },
      "outputs": [],
      "source": [
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, csv_file, label_column=None):\n",
        "        self.data_frame = pd.read_csv(csv_file)\n",
        "        self.label_column = label_column\n",
        "\n",
        "        self.data_frame = self.data_frame.drop(columns=['meteo_radiation_IR'], errors='ignore')\n",
        "\n",
        "        if self.label_column:\n",
        "            self.data_frame[self.label_column] = self.data_frame[self.label_column].astype(str)\n",
        "            self.labels = self.data_frame[self.label_column].map(class_mapping)\n",
        "            self.labels = self.labels.astype(int).values\n",
        "            self.features = self.data_frame.drop(columns=[self.label_column])\n",
        "        else:\n",
        "            self.features = self.data_frame\n",
        "            self.labels = None\n",
        "\n",
        "        self.encoders = {}\n",
        "        for col in self.features.columns:\n",
        "            if self.features[col].dtype == 'object' or not pd.api.types.is_numeric_dtype(self.features[col]):\n",
        "                encoder = LabelEncoder()\n",
        "                self.features[col] = encoder.fit_transform(self.features[col].astype(str))\n",
        "                self.encoders[col] = encoder\n",
        "\n",
        "        self.features = self.features.apply(pd.to_numeric, errors='coerce')\n",
        "        self.features = self.features.fillna(self.features.mean())\n",
        "        self.features = self.features.values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def bruh(self):\n",
        "        print(\"Unique raw values in label column:\", self.data_frame[self.label_column].unique())\n",
        "        print(\"Label dtype:\", self.labels.dtype if self.labels is not None else \"No labels\")\n",
        "        print(\"First 10 labels:\", self.labels[:10] if self.labels is not None else \"No labels\")\n",
        "\n",
        "        nan_counts = self.data_frame.isna().sum()\n",
        "        nan_columns = nan_counts[nan_counts > 0]  # Filter columns with NaN values\n",
        "        if len(nan_columns) > 0:\n",
        "            print(\"Columns with NaN values:\")\n",
        "            print(nan_columns)\n",
        "        else:\n",
        "            print(\"No NaN values in features.\")\n",
        "\n",
        "        print(\"NaN values in features:\", np.any(np.isnan(self.features)))\n",
        "        if self.labels is not None:\n",
        "            print(\"NaN values in labels:\", np.any(np.isnan(self.labels)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        features = torch.tensor(self.features[idx], dtype=torch.float32)\n",
        "        if self.labels is not None:\n",
        "            label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "            return features, label\n",
        "        return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FFjoiCD8DEdO"
      },
      "outputs": [],
      "source": [
        "def vector_to_class(x):\n",
        "  y = torch.argmax(x,axis=1)\n",
        "  return y\n",
        "\n",
        "def prediction_accuracy(predict,labels):\n",
        "  accuracy = (predict == labels).sum()/(labels.shape[0])\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "06oak9-KIjbM"
      },
      "outputs": [],
      "source": [
        "class_weights_tensor = torch.tensor([3.866946, 3.999680, 4.492605, 5.817200, 10.321409])\n",
        "\n",
        "class WeightedCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, class_weights):\n",
        "        super(WeightedCrossEntropyLoss, self).__init__()\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        return nn.CrossEntropyLoss(weight=self.class_weights)(y_pred, y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = TabularDataset(csv_file=train_temp_csv, label_column='piezo_groundwater_level_category')\n",
        "val_dataset = TabularDataset(csv_file=val_csv, label_column='piezo_groundwater_level_category')\n",
        "\n",
        "train_features = train_dataset.features\n",
        "val_features = val_dataset.features\n",
        "train_labels = train_dataset.labels\n",
        "val_labels = val_dataset.labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "YymWaTFtq-Ra",
        "outputId": "42cee079-db81-4136-aa7e-1a6f9e373b0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\ntabnet_model = TabNetClassifier(\\n    optimizer_fn=torch.optim.Adam,\\n    optimizer_params=dict(lr=2e-2),\\n    scheduler_params=dict(step_size=50, gamma=0.9),\\n    scheduler_fn=torch.optim.lr_scheduler.StepLR,\\n    mask_type=\"entmax\"\\n)\\n\\n# Replace loss function with the custom weighted loss\\ntabnet_model.loss_fn = WeightedCrossEntropyLoss(class_weights=class_weights_tensor)\\n\\n# Train TabNet\\ntabnet_model.fit(\\n    X_train=train_features, y_train=train_labels,\\n    eval_set=[(val_features, val_labels)],\\n    max_epochs=15, patience=20, batch_size=1024, virtual_batch_size=128\\n)\\n'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "tabnet_model = TabNetClassifier(\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params=dict(lr=2e-2),\n",
        "    scheduler_params=dict(step_size=50, gamma=0.9),\n",
        "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "    mask_type=\"entmax\"\n",
        ")\n",
        "\n",
        "# Replace loss function with the custom weighted loss\n",
        "tabnet_model.loss_fn = WeightedCrossEntropyLoss(class_weights=class_weights_tensor)\n",
        "\n",
        "# Train TabNet\n",
        "tabnet_model.fit(\n",
        "    X_train=train_features, y_train=train_labels,\n",
        "    eval_set=[(val_features, val_labels)],\n",
        "    max_epochs=15, patience=20, batch_size=1024, virtual_batch_size=128\n",
        ")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KB8rHhW_h1gM",
        "outputId": "eca1971d-ffae-4f09-996c-216d875d1efe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.5586800401570249\n"
          ]
        }
      ],
      "source": [
        "xgb_model = xgb.XGBClassifier(tree_method=\"gpu_hist\", reg_alpha=0.5)\n",
        "\n",
        "xgb_model.fit(train_dataset.features, train_dataset.labels)\n",
        "\n",
        "val_preds = xgb_model.predict(val_features)\n",
        "val_accuracy = accuracy_score(val_labels, val_preds)\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O44s1EiNioe8",
        "outputId": "6778593c-f143-4816-cb71-1afbb3f0f738"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091290 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 11368\n",
            "[LightGBM] [Info] Number of data points in the train set: 753470, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score -1.397299\n",
            "[LightGBM] [Info] Start training from score -1.367918\n",
            "[LightGBM] [Info] Start training from score -1.520861\n",
            "[LightGBM] [Info] Start training from score -1.771027\n",
            "[LightGBM] [Info] Start training from score -2.212582\n",
            "LightGBM Validation Accuracy: 0.4804365052944348\n"
          ]
        }
      ],
      "source": [
        "train_data = lgb.Dataset(train_features, label=train_labels)\n",
        "val_data = lgb.Dataset(val_features, label=val_labels, reference=train_data)\n",
        "\n",
        "# Set hyperparameters\n",
        "params = {\n",
        "    \"objective\": \"multiclass\",\n",
        "    \"num_class\": len(set(train_labels)),  # Number of unique classes\n",
        "    \"metric\": \"multi_logloss\",\n",
        "    \"boosting_type\": \"gbdt\",\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"max_depth\": 100,\n",
        "    \"num_leaves\": 64,\n",
        "    \"feature_fraction\": 0.8,\n",
        "    \"bagging_fraction\": 0.8,\n",
        "    \"bagging_freq\": 5,\n",
        "    \"lambda_l1\": 1,\n",
        "    \"lambda_l2\": 1,\n",
        "}\n",
        "\n",
        "# Train LightGBM\n",
        "lgb_model = lgb.train(\n",
        "    params=params,\n",
        "    train_set=train_data,\n",
        "    valid_sets=[train_data, val_data],\n",
        "    num_boost_round=250\n",
        ")\n",
        "\n",
        "# Validate predictions\n",
        "val_predictions = lgb_model.predict(val_features)\n",
        "val_predictions = val_predictions.argmax(axis=1)  # Convert probabilities to class indices\n",
        "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
        "print(f\"LightGBM Validation Accuracy: {val_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3Lfw-yri3cQ",
        "outputId": "018cb2c9-ea41-4353-e884-0073ed2352af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.3918683\ttest: 0.3341010\tbest: 0.3341010 (0)\ttotal: 3.14s\tremaining: 2m 33s\n",
            "49:\tlearn: 0.4900261\ttest: 0.3892607\tbest: 0.3895023 (46)\ttotal: 2m 8s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.3895022539\n",
            "bestIteration = 46\n",
            "\n",
            "Shrink model to first 47 iterations.\n",
            "CatBoost Validation Accuracy: 0.3895022539370464\n"
          ]
        }
      ],
      "source": [
        "catboost_model = CatBoostClassifier(\n",
        "    iterations=50,\n",
        "    learning_rate=0.01,\n",
        "    depth=10,\n",
        "    loss_function=\"MultiClass\",\n",
        "    eval_metric=\"Accuracy\",\n",
        "    model_size_reg=1,\n",
        "    random_seed=42,\n",
        "    use_best_model=True,\n",
        "    verbose=100\n",
        ")\n",
        "\n",
        "# Train CatBoost\n",
        "catboost_model.fit(\n",
        "    train_features, train_labels,\n",
        "    eval_set=(val_features, val_labels),\n",
        "    early_stopping_rounds=50\n",
        ")\n",
        "\n",
        "# Validate predictions\n",
        "val_predictions = catboost_model.predict(val_features)\n",
        "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
        "print(f\"CatBoost Validation Accuracy: {val_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA-KQzA1jKrZ",
        "outputId": "16224f8b-b3aa-47f3-e2ea-00eba024f981"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DecisionTree Validation Accuracy: 0.5965041455992418\n"
          ]
        }
      ],
      "source": [
        "tree = DecisionTreeClassifier()\n",
        "tree.fit(train_features, train_labels)\n",
        "\n",
        "# Calculate probabilities\n",
        "val_predictions = tree.predict(val_features)\n",
        "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
        "print(f\"DecisionTree Validation Accuracy: {val_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtGuoeJ6lfjX",
        "outputId": "6f68de58-fb37-45c4-d7f4-ef049df27b9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined Model Validation Accuracy: 0.5965041455992418\n",
            "Test results saved to 'test_results.csv'.\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Predict Probabilities for Validation Set\n",
        "# tabnet_probs = tabnet_model.predict_proba(val_features)\n",
        "xgb_probs = xgb_model.predict_proba(val_features)\n",
        "lgb_probs = lgb_model.predict(val_features)  # LightGBM probabilities\n",
        "#catboost_probs = catboost_model.predict_proba(val_features)\n",
        "tree_probs = tree.predict_proba(val_features)\n",
        "\n",
        "# Step 2: Combine Probabilities by Multiplying\n",
        "combined_probs = xgb_probs * lgb_probs * tree_probs #* catboost_probs\n",
        "combined_probs = combined_probs / combined_probs.sum(axis=1, keepdims=True)  # Normalize to ensure valid probabilities\n",
        "\n",
        "# Step 3: Make Final Predictions\n",
        "final_predictions = np.argmax(combined_probs, axis=1)\n",
        "\n",
        "# Step 4: Evaluate Combined Model\n",
        "val_accuracy = accuracy_score(val_labels, final_predictions)\n",
        "print(f\"Combined Model Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "# Step 5: Predict Probabilities for Test Set\n",
        "test_dataset = TabularDataset(csv_file=test_csv) # Load the test data\n",
        "\n",
        "# tabnet_test_probs = tabnet_model.predict_proba(test_features)\n",
        "xgb_test_probs = xgb_model.predict_proba(test_dataset.features)\n",
        "lgb_test_probs = lgb_model.predict(test_dataset.features)  # LightGBM probabilities\n",
        "#catboost_test_probs = catboost_model.predict_proba(test_dataset.features)\n",
        "tree_test_probs = tree.predict_proba(test_dataset.features)\n",
        "\n",
        "# Combine probabilities for the test set\n",
        "test_combined_probs = xgb_test_probs * lgb_test_probs * tree_test_probs #* catboost_test_probs\n",
        "test_combined_probs = test_combined_probs / test_combined_probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "# Step 6: Make Final Predictions for Test Set\n",
        "test_final_predictions = np.argmax(test_combined_probs, axis=1)\n",
        "\n",
        "# Decode predictions if needed (map numeric classes to original labels)\n",
        "decoded_predictions = [inverse_class_mapping[pred] for pred in test_final_predictions]\n",
        "\n",
        "# Step 7: Save Results to CSV\n",
        "with open('test_results.csv', mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['row_index', 'piezo_groundwater_level_category'])  # Write header\n",
        "    for id, pred_class in zip(test_dataset.data_frame['row_index'], decoded_predictions):\n",
        "        writer.writerow([id, pred_class])  # Write each ID and predicted class\n",
        "\n",
        "print(\"Test results saved to 'test_results.csv'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
